---
layout: post
title: "WIGO: General Game Playing AI"
description: ""
category:
tags:
- browser
- coffeescript
- ai
---
{% include JB/setup %}

## Demo
[http://dabbler0.github.io/wigo/](http://dabbler0.github.io/wigo/)

## Conception
This was a project for class, but I'm proud enough of it that I'm going to put it here. The idea was to play any game at all by reinforcement learning, since apparently this works!? We'll invent a general discription of discrete games and try to get a reinforcement learner to play them. Shortlist for now: 2048, blackjack, snake. Because this was a project for class and not really a hack-per-day, this will be a slightly shorter write-up.

<!--more-->

## Tech
Coffeescript, of course, in the browser, with some html5 canvas for nice-looking graphics.

## Hack
So, according to academia, the way that reinforcement learning is this:

  - You want to come up with some function `g:(state,move)->number` that sends game states to a "desirability" value that loosely reflects expected score from making that move.
  - To make this function, you train some kind of regressor on the state and move. But you don't just regress on the state that actually caused your score to increase: you also regress on the previous nodes, by some discount factor. So suppose you went along nodes `A-east->B-north->C-east->D` in the game state graph at got 20 points upon reaching state `D`. Then if you had a discount factor of 2, you would feed your regressor:

      - `C, east` mapped to `20`
      - `B, north` mapped to `10`
      - `A, east` mapped to `5`
  - At least, that's what you would do, ideally, But you can't, because that takes too much time, since your path history grows a lot. So instead what you do is you give each state a "desirability" value, which is computed as some kind of combination of its moves (different in Q- learning and in SARSA- learning). Then, _every time you make a move_, you feed to the regressor that your move mapped to the next node's desirability value. So in our previous example, `C` might be assigned a desirability value of 15 now (or something). Then, if we ever revisit `B` and move east, `B, east` will feed to the regressor that `B, east` mapped to 15/2. Then the desirability value for `B` would be updated, etc., and eventually the values will be propagated back.

I read most of this in academic papers and just implemented it. I made some example games, and it worked pretty well! You can check out snake, which is the most impressive. I'd like to come back to this project at some point and compare regression methods: [muse](http://dabbler0.github.io/hack-per-day/2014/12/14/muse/) has taught me some interesting techniques that I would like to see WIGO try.
