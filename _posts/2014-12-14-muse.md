---
layout: post
title: "Muse: Chord Recognition"
description: ""
category:
tags:
- music
- nw
- coffeescript
- ai
- hmm
- regression
---
{% include JB/setup %}

## Demo
No online demo, sorry! On github [here](https://github.com/dabbler0/muse).

## Conception
Yet another step towards autoremix. Today, I just want to do chord recognition. According to academia, this is not difficult; you do some kind of regression between chords and STFT frames, then you run an HMM over everything using that regressor as your emission probabilities. I figure that if I can get this working okay, then autoremix is simple; just do a Markov-chain chord progression generator and use clips from the songs to express the chords.

<!--more-->

## Tech
CoffeeScript and node.js. At first, I wanted tried to do this entirely in-browser, but when I tried to do analysis there I ran out of RAM pretty quick. Also, localStorage has restrictive size limits, so I found myself about to just make a server for my own local app, which seemed silly. Instead, I shifted over to node-webkit.

I'm also going to be using a gulp/browserify for the first time in a hack (I've been using grunt/requirejs for the past year or so). Grunt and requirejs seem to be getting a bunch of criticism while gulp and browserify are generally praised, so we'll see how it goes.

## Hack
Again, I started out entirely in-browser. So the first thing I wanted to do was make a clean wrapper for the web audio API. I just copy a bunch of stuff from my old speech recognition experiments and found some example code online for sound playing. I wrap these up in a browserify package and put them away for now.

I'm feeling a little unhackerish right now, so I'm going to start with some disciplined architecture for the stats stuff. Linear regressor first. Basic OOP: an `Estimator` has a `feed` and `estimate` method. First I implement stochastic gradient descent, taken from my [wigo](wigo) codebase. In accordance with "disciplined architecture," I write a corresponding unit test for it. The accuracy is not great: we are often off by up to 0.5 on inputs and outputs -5 to 5. So I'll write a static regressor that uses the method of normal equations. That works much better, within something like 1e-10, but is extremely slow. Perhaps that's just the trade-off for now?

Well, it turns out not, I did a little bit more research and found that you can actually do the method of normal equations incrementally, which saves me a ton of time and immeasurable amounts of memory. I actually could not find any description of how this was done, so I had to do some mathematical noodling myself. What you find is that:

  - The left-hand side of the equation, A-transpose times A, is fixed in dimensions.
  - When you add a rows to A, the elements of the product A-transpose times A increases based only on products from the new row. This lets us do incremental updates to the left side of the equation and consume a constant amount of memory when getting new info.

Okay, great! I could probably have found that in a textbook somewhere, but I did not have said textbook, so I feel very clever. Now I can analyse something like 4.5e6 data points in 12-dimensional space and still come within 1e-10 of reality. That's the regressor I'm going to use.

Okay, time to write the HMM. I'll bring in my code from the [classyfi](classyifi) codebase for the markov part. At this point I start to confuse myself a bit on the OOP. Do I want markov models to be `Estimator`s? I try to jam them into that role for a little bit and then give up. Markov models will just be a different thing. Then a `HiddenMarkovModel` contains a Markov model and an `Estimator`, and trains using the standard HMM DP algorithm, which I've done before for a school project. I test this out on some data; meh, 85% or so accuracy. It will have to do.

Tokenization time. I have been dabbling around in different sound tokenziation methods for a while, and there seem to be three big contenders for this role: the STFT, the MFCC, and a semitone filterbank. This project has to do with chords, so I am going to shoot for the semitone filterbank. More OOP goes down: a `Filterbank` can be applied to an arbitrary spectrum, and the `SemitoneFilterbank` extends it.

Okay, time to hook everything together... wait. I don't have any way to generate training data. So, time to make some chord markup interface. Here's how it will work: you use a scrubber to listen to songs backwards or forwards or whatever, then you pause at a spot you can input a chord change there. I'll bring in [chart.js](chartjs) for a bar chart of the semitone filterbank. Chart.js is a bit of pain to configure at first, because (a) it really likes to animate stuff and (b) I want it to make x-axis markings but not y-axis markings, and this is not supported. (a) was actually easy to solve, you can just tell it to stop, but (b) I was never able to fix. In the future I might dive into the chart.js code itself, but for now it's fine, whatever. At this point I try to dump recorded songs into localStorage and quickly hit quotas. I try feebly for a bit to get Chrome to raise the quotas just for me, then give up and decide to move to node-webkit. But there's a miracle! I am using browserify, which means that I am already on the nodejs module system, so this takes literally no work whatsoever. Thanks, browserify! I am not regretting my decision to abandon requirejs.

Now, I want some good quality audio to mark up. Is it legal to download songs off Youtube? Hopefully it is, because that is what I will be doing. I happen to have some reliable charts here for "On My Own" from Les Mis and "When I Grow Up" from Matilda, so I will go get those. There's a little pipeline I have to go through mp3->stereo pcm->mono pcm to get the data format I want. Then I can go grab some of my PCM code from (audioshift)[audioshift] to interpret this and pass it into my filterbank code. Things are a little shaky at first, as the pipelines are long, but I eventually get this working and I can listen to the songs forward and backward. That's pretty cool!

I come up with some lazy OOP for song chord chart storage formats. It's not perfect, but whatever. I'll invent a file system, too: chord charts will be stored as a list of pairs, each with an element containing the filterbank outputs, and an element containing the chord. In other words, exactly what we want to feed to the HMM. Great!

At this point I'm getting exhausted, so I just want to see something work. My heavy abstract frontloading seems not to have been a good idea, psychologically, as I have yet to see anything impressive from myself. So I ditch UI and just write a little script to interpret the frames output of my chord markup for "When I Grow Up." I run it through HMM training, then try to repredict the chords on the same song from the data -- it's honestly cheating, so it should be pretty accurate, right?

Nope. Experiment failed. Less than 50% accuracy.

I don't know what to say.

So, I think a lot of the problem here was to do with metrics I'm using to do linear regression. I will try dabbling in the MFCC, with different ranges of filterbanks, bigger polynomial combinations for the regressor, or with applying some kind of high/low pass filter first to split the sound. Experiments for another day. Right now I'm discouraged.

[wigo]: http://github.com/dabbler0/wigo
[classyfi]: http://dabbler0.github.io/hack-per-day/2014/04/23/classyfi/
[chartjs]: http://www.chartjs.org/
