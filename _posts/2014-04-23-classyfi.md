---
layout: post
title: "Classyfi"
description: ""
category: 
tags:
- natural_language_processing
- statistics
- classical_classification
- coffeescript
---
{% include JB/setup %}

## Demo
[http://dabbler0.github.io/classyfi](http://dabbler0.github.io/classyfi)

Can distinguish **Latin Text** from **English Text** with **99.0% accuracy**

Can distinguish **Charles Dickens** from **Jane Austen** with **73.5% accuracy**

## Conception
Back in middle school I made a [smoothed Markov Model classifier in Python][filesort] and met with great success. It was a little hacky and very special-cased, so I want to do a generalisation of it and port it to CoffeeScript.
<!--more-->

## Tech
This is going to be pure, environment-free CoffeeScript. Ideally, no dependencies.

## Hack
Okay, I'm writing a generalised library, so OOP is the name of the game. Base class is `Estimator`, which can `estimate` the probability of a sequence of tokens, and which we can `feed` a corpus of tokens for training. `MarkovModel` estends `Estimator`, and will know how to do [held out smoothing][hosmooth] with another `MarkovModel`.

This is all stuff I've done before, so I push this pretty quickly. The stages of a MarkovModel's lifecycle are as follows:
```
1. feed (tokenString)
    -> add (ngram) for ngram in tokenString
      -> recurse to children
2. normalize () # scale probabilities to one
  -> recurse to children
3. estimate (tokenString)
    -> getProbability (ngram) for ngram in tokenString
      -> recurse to children
```
The `MarkovModel` class has a tree structure, such that we can reach any given state `[a1,a2..an]` by accessing the `a1->a2..->an`th children of each node.

Oky, time to implement [held out smoothing][hosmooth]. I like this algorithm mainly because it's simple and intuitive. I was introduced by my favourite textbook, [Foundations of Statistical Natural Language Processing][foundations] by Manning and Schutze. The basic mechanic is as follows:
```
1. Lump tokens together that have the same count (e.g. we have a bucket of all unseen tokens, a bucket of all tokens seen exactly once, etc.)
2. Count buckets as a whole in the Held Out data (e.g. if A are the tokens seen in training, the bucket count for A would be the number of times we see _any_ token from A in the Held Out data)
3. Redistribute counts evenly within buckets.
```

I've implemented this in both Python and Java before so this was no big deal. To do things a little cleaner, in this implementation I'm wrapping it in some OOP -- a `SmoothedMarkovModel` extends an `Estimator`, and has the same `feed` and `estimate` functions. When we `feed` it it will send tokens to the `training` or `held out` corpora depending on which is smaller. So now we add a stage in its lifecycle:
```
1. feed
2. smooth ()
  -> training.smoothHO(heldout) averaged with heldout.smoothHO(training)
3. normalize
4. estimate
```

The extra steps will just be done if necessary right before `estimate()`.

I kind of transiently put down some OOP for a `Classifier` and a `Category`, for a real full-fledged [Bayesian classification engine][bayesian], but then when writing tests decided to put that idea down.

Okay, time to write tests. My first test is going to be to try to identify Latin text against English text using letter trigrams. For training data, I'll use Cicero's Oratio In Catilinam I concatenated with Caesar's Commentarii de Bello Gallico. For English training data I'll scrape three random news articles from Google News. I ran this training and put up a simple node.js `readline` interface classifying input lines.

Unfortunately this did not seem to be very accurate.

My first thought at this point is that I was just measuring the wrong thing. I have had bad experiences with letter trigrams in the past. So I shifted to word bigrams, trying to identify Jane Austen's writing from Charles Dickens'. For training data I'll snatch _Pride and Prejudice_ and _Great Expectations_ from [Project Gutenberg][gutenberg].

For visualization purposes, I'll implement a simple `generateRandom` method on the `MarkovModel` to generate a weighted random string with the Markov model transition probabilities. For word bigrams, I'm also going to need to get a little more serious about my tokenizer. I'll write a simple script to fetch the _n_ most common tokens from a corpus, and that'll form the alphabet. This all goes up fine (if low-performance). I'm going to try to write some rigorous tests this time, testing [average surprise][avgsurprise] (cross entropy) and accuracy. For testing data I'll pull Charles Dickens' _Oliver Twist_ and Jane Austen's _Sense and Sensibility_.

When I begin to run these tests, I immediately notice that there are huge performance issues with estimation. I dig through the code a bit and find a dumb typo causing it to re-smooth every time we call `estimate`. I fix this and performance improves incredibly, and I can run tests in under a second. Great! Unfortunately, accuracy is very low. I don't even want to look at the percentages; my logs of `SUCCESS` and `FAIL` are honestly mostly `FAIL`.

So for debugging I'm going to log a serialization of the models. Reading the serialization makes me immediately realize that something is wrong -- most probabilities are the same. Drat! This is also a dumb bug in my `SmoothedMarkovModel` `feed` implementation; it's feeding everything to the `training` set, so smoothing ends up assigning everything equal probability. I fix this, and my results go up to about **65% accuracy**. This is a little disappointing, so I'm going to try what I did in middle school: cut down on the information.

In my middle school experiments I found that **unigram counting is much more accurate than bigram for sparse data**. So I replace this and my identification accuracy goes up to around **73%**. This is good, although not super impressive. Now that I've fixed my smoothing bug, though, maybe I can go back to English vs. Latin and identify things well.

I go back and plot my new library back into my old example code. The interactive shell yeilds good results. But I want to do some rigorous testing, so I'm grabbing Cicero's Third Catilinarian and random book from Project Gutenberg for testing data. For my english data I'm going to enlarge the data set by grabbing a random book from Project Gutenberg instead of using random news articles. I run the tests (with letter trigrams), and voila! We have **99% accuracy**! Woohoo!

## Launch
I'll draw up a simple demo (static, with bootstrap ui) for Github pages.

[filesort]: https://www.assembla.com/code/dab_sci_fair/subversion/nodes
[hosmooth]: http://en.wikipedia.org/wiki/Cross-validation_(statistics)
[foundations]: http://nlp.stanford.edu/fsnlp/
[gutenberg]: http://www.gutenberg.org/
[avgsurprise]: http://en.wikipedia.org/wiki/Cross-entropy
[bayesian]: http://en.wikipedia.org/wiki/Naive_Bayes_classifier
