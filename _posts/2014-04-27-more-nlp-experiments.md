---
layout: post
title: "More NLP Experiments"
description: ""
category: 
tags: []
---
{% include JB/setup %}

## Demo

## Conception
Sorry I haven't posted in a while. I've been through a bunch of different failed signal-processing hacks (I was trying to do automatic music dictation and [TD-PSOLA][tdpsola] vocoding), which I might amalgamate into a collection sometime later. Anyway, what I'll show today is a couple other NLP techniques I wanted to try to implement in CoffeeScript. These are:
  - Unsupervised classification with [Estimation-Maximiziation][EM]
  - [Logistic regression][lregression] as an alternative to Naive Bayes.

## Tech
These will be additions to the [Classyfi] library I posted earlier. Thus, they will be pure, environment-free CoffeeScript.

## Hack
The first thing I'll try to implement today is unsupervised classification. The basic problem here is to separate two different "types" of text given a **untagged** corpus. Latin and English from last time seemed substantially different, so my goal with this first hack will be to separate English from Latin without any training data.

The core mechanic of Estimation-Maximization is as follows:
```
1. Initialize random model
2. Until we are satisfied,
  1. Classify the corpus using the current model.
  2. Re-train the model on the classified corpus.
```
This will eventually converge to some maximum probability model. So, hacking -- first thing to do is to add randomization to the `MarkovModel` and `SmoothedMarkovModel` OOP -- that's easy. Then let's write the code for a single EM step -- this is also easy, since we have classification and training functions already built in. So far, so good. Now we write the code for the whole process, first randomizing and then running the EM code until we converge. Let's define convergence in terms of an epsilon:
```
converge <=> |last_probability - current_probabilty| < epsilon
```
Great. I run this experiment and find that for some reason the first iteration is highly probable compared to the other iterations -- this should not be happening; probability should always increase with an iteration. This was a dumb bug -- I had simply forgotten to normalize the model at the beginning of the process. So I normalized it, and everything went well.

Some results (GROUP 1 and GROUP 2 were generated by EM; ENGLISH and LATIN are true tags; numbers indicate number of documents in the with both tags. More imbalance is better.):
```
Test 1
------
         GROUP 1   GROUP 2
ENGLISH  356       0
LATIN    3         142

Test 2
------
         GROUP 1   GROUP 2
ENGLISH  3         353
LATIN    143       2

Test 3
------
         GROUP 1   GROUP 2
ENGLISH  356       0
LATIN    3         142
```

This looks pretty good!

Okay, second thing to implement today is logistic regression. Logistic regression is a discriminative model based on the sigmoid function. Today I will only implement binary logistic regression. The core assumption of logistic regression is this:
```
P(y|x) = S(B(x))
```
where `S` is the [sigmoid] function and B is a linear function.

Training is done with [MLE][MLE] scoring, and optimized using an iterative numeric method like [Stochastic Gradient Descent][gdescent] or [Brent's Approximation][brent]. I am going to use a very simplified form of Stochastic Gradient descent for my training.

Boilerplate OOP: I'll add class `LogisticModel` extending `Estimator`. My input and output vectors will be the standard neural-net vectors for text -- unigram word counts. This makes my logistic model a little bit less powerful (generalisable) than the Markov Model before, but ultimately a `LogisticModel` can be used as the transition probability estimator for a [Conditional Random Field][crf], which is more powerful than a Markov Model.

Before I link `LogisticModel` into the `Estimator` interface, I'll deal with points the way that the `LogisticModel` naturally wants to -- accepting n-dimensional points with classifications and outputting the same. So, operations we can perform on a  `LogisticModel` are `feedPoint(point)`, `train()`, and `estimatePoint(vector)`. `train()` will take all the points given in `feedPoint()`, perform gradient descent on them to get the maximum-likelyhood model parameters, and then `estimatePoint(vector)` will apply the sigmoid function to estimate the probability using those parameters.

My formula for stochastic gradient descent I'll copy from [these lecture notes][lnotes]. I was pretty tired writing this, so I ran into a couple dumb typos that gave mysterious NaN answers. But I worked these out. Okay, so I have the code, but I have no idea whether it works as an estimator or not, so I need to link this into the `Estimator` interface and run some tests.

Linking into the `Estimator` interface should not be difficult -- I'll just put simple wrapper functions up that will populate a vector with unigram counts. Great, that's done.

Time to write the tests. I'll be scoring Charles Dickens vs. Jane Austen again, with the same training and test data as earlier, to test this against my HO-smoothed Naive Bayes unigram model from earlier. I run the tests, and the results are a little strange. We can identify Dickens as Dickens with **80%** accuracy, but Austen is identified as Dickens over **35%** of the time (**65% accuracy**). That's no good!

One thing I didn't incorporate into my estimator -- a bias term. This is actually necessary for mathematical rigor -- we need an element of the input vector that is **constant**, so as to make our linear combination function `B` general (otherwise all the linear combination functions would pass through (0,0,0...0)). So I added this in, and accuracy did not change much. I ramped up sample size to 1000, and the number of iterations for stochastic gradient descent, and accuracy went up slightly -- Dickens positive **99%** and Austen positive **70%**, for a total accuracy of **87%**. There seems to be a weird bias towards Dickens here, and I am not sure why. At any rate, this performs much worse (and also trains slower) than the smoothed NB classifier, so I'm ditching this concept for now.
